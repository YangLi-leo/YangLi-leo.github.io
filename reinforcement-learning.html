<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How Reinforcement Learning Unlocks LLM Agents Capabilities - Yang Li</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <style>
        /* Âü∫Êú¨Ê†∑ÂºèËÆæÁΩÆ */
        body {
            font-family: 'Fira Code', 'Source Code Pro', monospace;
            background-color: #1e1e1e;
            color: #d4d4d4;
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        
        /* ÂØºËà™ÈìæÊé• */
        .nav-link {
            display: inline-block;
            margin-bottom: 30px;
            padding: 8px 0;
            font-size: 1.1em;
            color: #569cd6;
            text-decoration: none;
            border-bottom: 1px solid transparent;
        }
        
        .nav-link:hover {
            border-bottom: 1px solid #569cd6;
        }
        
        /* PythonÈ£éÊ†ºÂÖÉÁ¥† */
        .keyword {
            color: #569cd6;
        }
        
        .function {
            color: #dcdcaa;
        }
        
        .comment {
            color: #6a9955;
            font-style: italic;
        }
        
        .string {
            color: #ce9178;
        }
        
        .variable {
            color: #9cdcfe;
        }
        
        /* È°µÈù¢Ê†áÈ¢ò‰ª£Á†ÅÂùó */
        .header-code {
            background-color: #252526;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 30px;
            border-left: 3px solid #569cd6;
        }
        
        /* ÊñáÁ´†ÂÆπÂô® */
        .article-container {
            background-color: #252526;
            border-radius: 6px;
            padding: 30px;
            margin-top: 20px;
            position: relative;
            border-left: 3px solid #569cd6;
        }
        
        /* ÊñáÁ´†Êó•Êúü */
        .article-date {
            color: #6a9955;
            font-size: 0.95em;
            margin-bottom: 30px;
            display: block;
        }
        
        /* ÂçöÂÆ¢ÂÜÖÂÆπÊ†∑Âºè */
        .blog-content {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.8;
            color: #e0e0e0;
        }
        
        .blog-content h1 {
            color: #4ec9b0;
            font-size: 2em;
            margin-top: 40px;
            margin-bottom: 20px;
            font-weight: 600;
        }
        
        .blog-content h2 {
            color: #4ec9b0;
            font-size: 1.6em;
            margin-top: 40px;
            margin-bottom: 20px;
            font-weight: 500;
            border-bottom: 1px solid #333;
            padding-bottom: 10px;
        }
        
        .blog-content p {
            margin-bottom: 20px;
        }
        
        .blog-content strong {
            color: #ce9178;
            font-weight: 600;
        }
        
        .blog-content em {
            color: #dcdcaa;
            font-style: italic;
        }
        
        /* ÂºïÁî®Ê†∑Âºè */
        .blog-content blockquote {
            margin: 30px 0;
            padding: 20px;
            background-color: #2d2d2d;
            border-left: 3px solid #569cd6;
        }
        
        .blog-content blockquote p {
            margin: 0;
        }
        
        /* ‰æßËæπÊ≥®ÈáäÊ†∑Âºè */
        .blog-content aside {
            margin: 30px 0;
            padding: 20px;
            background-color: #2d2d2d;
            border-left: 3px solid #4ec9b0;
            font-style: italic;
        }
        
        /* Ê†áÂáÜ‰ª£Á†ÅÂùóÊ†∑Âºè */
        .blog-content pre {
            background-color: #1e1e1e;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            font-family: 'Fira Code', 'Source Code Pro', monospace;
            border: 1px solid #333;
            margin: 20px 0;
        }
        
        /* Á¥ßÂáëÂûãÊµÅÁ®ãÂõæ‰ª£Á†ÅÂùóÊ†∑Âºè */
        .compact-diagram {
            background-color: #1e1e1e;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
            font-family: 'Fira Code', 'Source Code Pro', monospace;
            border: 1px solid #333;
            margin: 20px 0;
            font-size: 0.8em;
            line-height: 1.2;
            white-space: pre;
        }
        
        .blog-content code {
            font-family: 'Fira Code', 'Source Code Pro', monospace;
            background-color: #2d2d2d;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        
        /* ÂõæÁâáÊ†∑Âºè */
        .blog-content img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 30px auto;
            border-radius: 4px;
        }
        
        /* ÂàÜÈöîÁ∫øÊ†∑Âºè */
        .blog-content hr {
            border: none;
            height: 1px;
            background-color: #333;
            margin: 40px 0;
        }
        
        /* ÂàóË°®Ê†∑Âºè */
        .blog-content ul, .blog-content ol {
            margin-bottom: 20px;
            padding-left: 20px;
        }
        
        .blog-content li {
            margin-bottom: 8px;
        }
        
        /* È°µËÑöÊ†∑Âºè */
        footer {
            margin-top: 60px;
            padding-top: 20px;
            border-top: 1px solid #333;
            text-align: center;
            font-size: 0.9em;
            color: #6e7681;
        }
        
        /* ÂìçÂ∫îÂºèËÆæËÆ° */
        @media screen and (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }
            
            .article-container {
                padding: 25px 15px 25px 15px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- ÂØºËà™ÈìæÊé• -->
        <a href="papers.html" class="nav-link"><span class="keyword">def</span> <span class="function">go_back</span>(): <span class="keyword">return</span> <span class="string">"Paper Reading"</span></a>
        
        <!-- È°µÈù¢Ê†áÈ¢ò‰ª£Á†ÅÂùó -->
        <div class="header-code">
            <span class="keyword">class</span> <span class="function">BlogPost</span>:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">def</span> <span class="function">__init__</span>(self):
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.author = <span class="string">"Yang Li"</span>
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.category = <span class="string">"AI Research"</span>
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.date = <span class="string">"May 11, 2025"</span>
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.tags = [<span class="string">"Reinforcement Learning"</span>, <span class="string">"LLM"</span>, <span class="string">"Agent Capabilities"</span>]
        </div>
        
        <!-- ÊñáÁ´†ÂÆπÂô® -->
        <div class="article-container">
            <span class="article-date">May 11, 2025</span>
            
            <!-- ÂçöÂÆ¢ÂÜÖÂÆπÂå∫Âüü - ‰ΩøÁî®Áî®Êà∑ÂéüÂßãÂÜÖÂÆπ -->
            <div class="blog-content">
                <h1>How Reinforcement Learning Unlocks LLM Agents Capabilities: Insights from Search Optimization</h1>
                
                <aside>
                üí° End-to-end reinforcement learning will be the key to unlocking the next level of LLM capabilities.
                </aside>
                
                <p>The rapid progress of large language models over the past eight months has been astonishing, marked by significant milestones such as o1, R1, Gemini 2.5 Pro, Sonnet 3.7, o3, and o4. We've witnessed their transformation from basic chatbots into powerful agents capable of addressing complex, real-world problems. I still remember struggling with GPT-4 at the end of 2023, where prompting external searches was cumbersome and unreliable. However, the recent release of o4-mini showcased vastly improved search capabilities, transparently revealing the model's thought processes in real-time.</p>
                
                <p>DeepSeek-R1 notably highlights that, with sufficiently intelligent base models, reinforcement learning (RL) can replace supervised fine-tuning (SFT) altogether‚Äîallowing models to autonomously learn optimal behaviors. From my perspective, RL unlocks capabilities that go far beyond incremental benchmark improvements‚Äîit significantly enhances models' practical problem-solving abilities.</p>
                
                <p>I'll start by revisiting the influential 2021 paper <strong>WebGPT</strong>, which pioneered applying reinforcement learning (RL) to enhance LLM-driven search capabilities. After that, we'll explore recent developments in LLM-based search, where two main approaches dominate today: framework-driven methods combined with retrieval-augmented generation (RAG)‚Äîsuch as Search-o1‚Äîand RL-based methods, enabling models to autonomously refine search strategies and proactively use tools. Our discussion will focus on the RL-driven approach, highlighting why it emerges as the more effective strategy.</p>
                
                <h2>WebGPT(<a href="https://arxiv.org/pdf/2112.09332">https://arxiv.org/pdf/2112.09332</a>)</h2>
                
                <p>This paper was genuinely ahead of its time, and even now‚Äînearly four years later‚Äîit remains highly insightful, innovatively enabling LLMs to leverage search engines for accessing up-to-date documents and mimicking human-like interactions. Rather than directly calling the Bing API, the authors developed a text-based browser simulator supporting actions such as Search, Click, Scroll, and Quote. Starting with GPT-3 (760M/13B/175B), initially lacking any inherent tool-use ability, they utilized reinforcement learning to help the model autonomously discover effective strategies.</p>
                
                <hr>
                
                <!-- Á¥ßÂáëÂûãÊµÅÁ®ãÂõæ‰ª£Á†ÅÂùó -->
                <pre class="compact-diagram">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Phase I: Model Building    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇHuman Demos  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇSplit into "State‚ÜíAction"    ‚îÇ
‚îÇ(state-action)‚îÇ    ‚îÇsamples                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇFine-tune BC ‚Üí get policy œÄ‚ÇÄ‚îÇ
‚îÇ(Supervised learning)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAnswer Compare‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇTrain Reward Model (RM)     ‚îÇ 
‚îÇ(Q,A‚ÇÅ,A‚ÇÇ,lbl) ‚îÇ    ‚îÇ(Pairwise logistic loss)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇObtain scorer R(Q, A)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Phase II: RL Training        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ 
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇInitialize œÄ ‚Üê œÄ‚ÇÄ(BC-SFT)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ„ÄêRepeat until convergence„Äë ‚îÇ
‚îÇ1. Rollout: execute œÄ       ‚îÇ
‚îÇ   ‚Üí produce answer A       ‚îÇ
‚îÇ2. Reward: r = R(Q, A)      ‚îÇ
‚îÇ3. PPO update:              ‚îÇ
‚îÇ   max E[r] ‚àí Œ≤¬∑KL(œÄ||œÄ‚ÇÄ)   ‚îÇ
‚îÇ4. Update policy œÄ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇGet optimized policy œÄ*     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Inference & Deployment     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇOptional: Best-of-n Reranking‚îÇ
‚îÇ‚Ä¢ Sample n answers {A·µ¢}     ‚îÇ
‚îÇ‚Ä¢ Score each: r·µ¢ = R(Q, A·µ¢) ‚îÇ
‚îÇ‚Ä¢ Select A* = argmax·µ¢ r·µ¢    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>
                
                <ul>
                    <li>In Phase I (Model Building), human operators first generated state-action demonstrations through the text-based browser. These demonstrations were then used to perform behavior cloning (supervised learning), resulting in an initial policy œÄ‚ÇÄ. Subsequently, human annotators compared pairs of answers to train a Reward Model (RM), enabling automatic evaluation of answer quality for RL training.</li>
                    <li>Phase II (Reinforcement Learning) started from the initial policy œÄ‚ÇÄ and employed PPO to iteratively optimize the model's behavior within the simulated browser environment, eventually yielding the refined policy œÄ*. Finally, during the Inference and Deployment phase, test-time scaling was applied: multiple candidate answers were generated, re-ranked using the RM, and the best answer was selected for output.</li>
                </ul>
                
                <hr>
                
                <p>It's still unclear whether having LLMs directly mimic human browser usage is the optimal approach for information retrieval. However, leveraging reinforcement learning to enable LLMs to autonomously develop search strategies is undoubtedly innovative. Fundamentally, this approach explores how RL can empower LLMs to perform tasks that extend beyond their pre-trained knowledge.</p>
                
                <p>So, let's move on to the 'Modern' ways to do search via llm.</p>
                
                <h2>ReSearchÔºà<a href="https://arxiv.org/pdf/2503.19470">https://arxiv.org/pdf/2503.19470</a>Ôºâ</h2>
                
                <blockquote>
                    <p><em>Models used in this paper: Qwen2.5-7B(Instruct), Qwen2.5-32B(Instruct)</em></p>
                </blockquote>
                
                <p>This paper sets the foundational tone for most of the subsequent research. Let's first understand its core logic step-by-step through the provided workflow diagram. Notably, it does not provide supervised data for the reasoning steps. Instead, it leverages reinforcement learning (specifically GRPO) to encourage the model to autonomously reason with integrated search actions.</p>
                
                <p><img src="assets/images/blog/LUM_Policy__E.png" alt="LUM Policy  E.png"></p>
                
                <p>Because of the GRPO method, each question triggers multiple sampled trajectories (rollouts) that interleave &lt;think&gt; and &lt;search&gt; tags. Whenever the model encounters a &lt;/search&gt; tag, it performs a retrieval action, and the retrieved results are incorporated back into the reasoning chain. This iterative Think ‚Üî Search loop continues until an &lt;answer&gt; is produced, making search genuinely intertwined with reasoning, rather than merely appending search results. By iteratively performing sampling, scoring, and updating through GRPO until convergence, the policy eventually learns when to search, what to search for, and how to effectively utilize retrieved information, entirely without human-annotated search examples.</p>
                
                <hr>
                
                <p>Having covered the basic logic, we now discuss several intriguing details highlighted in this paper‚Äîdetails frequently revisited by later research:</p>
                
                <ul>
                    <li>After reinforcement learning, models naturally identify and correct mistakes, enabling self-driven re-searching.</li>
                </ul>
                
                <p><img src="assets/images/blog/Screen_Shot_2025-05-10_at_21.44.52_PM.png" alt="Screen Shot 2025-05-10 at 21.44.52 PM.png"></p>
                
                <ul>
                    <li>Even though the model was exclusively trained on the <em>MuSiQue</em> dataset, it generalizes remarkably well across multiple test sets, indicating the broad applicability of the ReSearch framework.</li>
                    <li>For smaller 7B models, the presence or absence of instruction-tuning shows limited differences. However, for larger 32B models, reinforcement learning significantly enhances performance, leading to notable gaps between base and instruction-tuned models.</li>
                    <li>Larger 32B models initially rely heavily on inherent knowledge, producing longer responses without frequently resorting to retrieval. But as training progresses, driven by reinforcement signals, they gradually begin leveraging search tools more actively, thus reducing dependency on internal knowledge.</li>
                </ul>
                
                <h2>R1-Searcher(<a href="https://arxiv.org/pdf/2503.05592">https://arxiv.org/pdf/2503.05592</a>)</h2>
                
                <blockquote>
                    <p><em>Models used in this paper: Llama-3.1-8B-Instruct, Qwen-2.5-7B-Base</em></p>
                </blockquote>
                
                <p>Essentially, this paper is an extension of <em>ReSearch</em>, but it brings innovative modifications in the training approach and provides more detailed insights into reinforcement learning (RL).</p>
                
                <p>The RL training is divided into two distinct stages:</p>
                
                <ul>
                    <li><strong>Stage-0: Pilot Run (Difficulty Labeling):</strong><br>
                    The authors first run all questions locally using an untrained Qwen-2.5-7B-Instruct model to record how many searches each question requires. Questions are then categorized into Easy, Medium, and Hard.</li>
                    <li><strong>Stage-1: "Learning to Search":</strong><br>
                    Using only Medium-level questions (HotpotQA 200 + 2Wiki 150), the model is trained with rewards emphasizing retrieval usage (<strong>Retrieval Reward</strong>) and proper response formatting (<strong>Format Reward</strong>). This allows the model to initially grasp <em>when</em> and <em>how</em> to effectively initiate search requests.</li>
                    <li><strong>Stage-2: "Learning to Answer":</strong><br>
                    Incorporating Hard-level questions (HotpotQA 2.5k + 2Wiki 2.5k), mixed with Medium-level questions, the reward shifts to prioritizing answer accuracy (<strong>F1/CEM</strong>) while penalizing format errors (<strong>Format Penalty</strong>). This phase encourages the model to produce high-quality answers based on retrieved information.</li>
                </ul>
                
                <p>Both training phases utilize the <strong>Reinforce++</strong> algorithm. Interestingly, later research has observed notable differences between Reinforce++ and GRPO. GRPO tends to encourage models to produce longer outputs and conduct more frequent searches, resulting in stronger generalization. Conversely, Reinforce++ leads to concise responses, faster convergence, and stronger performance within known domains. On average, their performance is comparable; the choice between them depends on whether your task emphasizes maximizing performance within a specific domain or achieving broad generalization.</p>
                
                <hr>
                
                <p>Some noteworthy results emerged from the experiments:</p>
                
                <ul>
                    <li>Compared to purely supervised fine-tuned (SFT) models, RL-trained models tend to call external search tools more proactively when unsure about answers, whereas SFT models rely heavily on their inherent knowledge‚Äîeven at the risk of providing incorrect responses.</li>
                </ul>
                
                <p><img src="assets/images/blog/Screen_Shot_2025-05-10_at_23.46.19_PM.png" alt="Screen Shot 2025-05-10 at 23.46.19 PM.png"></p>
                
                <ul>
                    <li>Remarkably, although the R1-Searcher was trained only on a local Wikipedia corpus, when evaluated on the completely unseen Bamboogle dataset using live Google API calls, the online Qwen-Base-RL achieved a +18.2% improvement in CEM over local retrieval and outperformed the 32B Search-o1 model by +11.4%. The fact that R1-Searcher surpasses Search-o1 is particularly noteworthy. Search-o1 is an agentic RAG method combined with reason-in-documents (primarily at inference), suggesting that RL-driven approaches can outperform carefully hand-crafted frameworks.</li>
                </ul>
                
                <p><img src="assets/images/blog/Screen_Shot_2025-05-10_at_22.40.05_PM.png" alt="Screen Shot 2025-05-10 at 22.40.05 PM.png"></p>
                
                <aside>
                üëÄ However, both ReSearch and R1-Searcher share a common limitation highlighted in the papers: <em>These methods typically employ static, local textual corpora such as Wikipedia and fail to capture the complexities of real-world interactions.</em><br><br>This indicates uncertainty regarding their performance in real-world scenarios and emphasizes the need to establish new, more realistic benchmarks for evaluating LLM search capabilities.
                </aside>
                
                <h1>Conclusion</h1>
                
                <p>This blog focuses primarily on sharing the latest architectures adopted by large language models (LLMs) when performing search tasks. Future posts will introduce even newer architectures. Although the architectures are becoming increasingly sophisticated, the underlying reinforcement learning algorithm remains GRPO; however, the innovative designs in reward functions deserve special attention. In subsequent blogs, we will specifically concentrate on discussing these reward function innovations.</p>
                
                <p>On one hand, we observe clear performance gains driven by architectural changes. On the other hand, I believe the detailed RL insights presented in these papers are even more crucial. From these examples, it's evident that reinforcement learning is highly effective in guiding models to autonomously explore optimal search strategies. Undoubtedly, this is a promising research direction, and we will continue to closely follow its developments.</p>
            </div>
        </div>
        
        <footer>
            <span class="comment"># ¬© 2025 Yang Li. All rights reserved.</span>
        </footer>
    </div>
</body>
</html>
